# ucbmlai-a51-driver-coupon-analysis

This repository analyzes a UCI survey dataset to understand which drivers are likely to accept time-limited coupons (restaurant, coffee house, bar, take-away, etc.). The project applies exploratory data analysis, visualization, and simple probabilistic modeling to identify factors that correlate with coupon acceptance.

Summary of findings
- Coupon acceptance correlates with situational context (destination alignment and time of day) and coupon type — for example, bar coupons show lower acceptance when children are present.
- After cleaning, the dataset contains ~12,000 observations and 25 features; the analysis removes duplicates and drops a column (`car`) that had ~99% missing values.

Notebook
- The full analysis is in the Jupyter notebook: `prompt.ipynb` (root). Consider moving and renaming it to `notebooks/assignment5-driver-coupon-analysis.ipynb` for clarity.

Quick start
1. Create a virtual environment (recommended):

```bash
# ucbmlai-a51-driver-coupon-analysis

This repository contains an exploratory analysis of a survey-style dataset used to study drivers' likelihood of accepting time-limited coupons (restaurant, coffee house, bar, take-away, etc.). The project focuses on data cleaning, exploratory data analysis (EDA), visualizations, and straightforward subgroup analyses to surface factors correlated with coupon acceptance.

**Quick Links**
- **Assignment instructions:** `assignment_instructions.md`
- **Data dictionary:** `coupons_data_dictionary.md`
- **Notebook:** `prompt.ipynb` (root)

**Quick Start**
- **Create a virtual environment:**

```bash
python -m venv .venv
source .venv/bin/activate
```

- **Install required packages (minimal):**

```bash
python -m pip install pandas numpy matplotlib seaborn jupyterlab
```

- **Run the dataset summary script:**

```bash
python src/describe_columns.py --csv data/raw/coupons.csv --max-values 10
```

- **Open the analysis notebook:**

```bash
jupyter lab prompt.ipynb
```

**Data files**
- **Raw:** `data/raw/coupons.csv`
- **Processed / cleaned (generated by notebook):** `data/processed/coupons_cleaned.csv` (may be regenerated by the notebook)

**What’s included**
- **`prompt.ipynb`**: analysis notebook containing data loading, cleaning steps (deduplication, dropping `car` column, dropping rows with remaining NaNs), EDA plots, and findings. Note: earlier versions contained VS Code cell wrappers; if you experience rendering issues, open the notebook in JupyterLab and re-save it to standardize the format.
- **`src/describe_columns.py`**: small helper script that prints per-column unique counts, missing counts, and top value frequencies for a CSV file.
- **`assignment_instructions.md`**: extracted assignment instructions and tasks.
- **`coupons_data_dictionary.md`**: data dictionary describing each column in `coupons.csv`.
- **`images/`**: exported figures generated by the notebook.

**Usage notes & recommendations**
- **Relative links:** The notebook and markdown files use relative links (e.g., `assignment_instructions.md`) so previews in VS Code and GitHub will open targets correctly. If you move files into `docs/` or another folder, update links accordingly.
- **Reproducibility:** Add a `requirements.txt` (e.g., `pip freeze > requirements.txt`) or an `environment.yml` to pin dependency versions for reproducible runs.
- **Notebook portability:** If your `prompt.ipynb` includes VS Code-wrapped cells, open and re-save it in JupyterLab to produce a standard `.ipynb` compatible with Binder and other viewers.

**Suggested next steps**
- Move notebooks to `notebooks/` and processed data to `data/processed/` for clearer separation of raw/derived artifacts.
- Add `requirements.txt` and a short `CONTRIBUTING.md` with commands to reproduce the cleaned dataset and regenerate figures.

If you want, I can also (a) convert and save `prompt.ipynb` to a standard Jupyter format in `notebooks/`, (b) create a `requirements.txt`, and (c) move the processed data into `data/processed/`. Tell me which of these you want me to do next.
